{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451051c-58c7-4fde-ac50-50817a9b1691",
   "metadata": {},
   "outputs": [],
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce600131-1861-428d-9f94-a8ab8cca49ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul 19 09:48:00 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:81:00.0  On |                  N/A |\n",
      "|  0%   46C    P3            140W /  370W |       2MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f694413a-04f0-42ed-953c-a033bef36152",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 321414                                                                                                                                                                                                                                                                                                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43d2683-ec61-438a-93f1-9bb560579628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d5bdd4-a1c3-4b51-9573-fe3dda98643d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/upb/users/b/balram/profiles/unix/cs/.conda/envs/kg_pipeline/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import json\n",
    "import jsonlines\n",
    "import re\n",
    "import time\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = True  # speedup\n",
    "\n",
    "def setup_model(model_id=\"meta-llama/Meta-Llama-3-8B\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False  # optional; can toggle True if preferred\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    return pipe, tokenizer\n",
    "\n",
    "def load_prompts(filepath):\n",
    "    with jsonlines.open(filepath) as reader:\n",
    "        return list(reader)\n",
    "\n",
    "def generate_text(generator, tokenizer, prompts, max_new_tokens=512):\n",
    "    dataset = Dataset.from_dict({\"text\": prompts})\n",
    "    outputs = generator(\n",
    "        dataset[\"text\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        truncation=True,\n",
    "        num_return_sequences=2,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    # ✅ Flatten if it's a list of lists\n",
    "    if isinstance(outputs[0], list):\n",
    "        outputs = [item for sublist in outputs for item in sublist]\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def extract_test_outputs(response):\n",
    "    outputs = []\n",
    "    if response and len(response) > 0:\n",
    "        for res in response:\n",
    "            generated_text = res.get('generated_text', '')\n",
    "            match = re.search(r'Test Output:\\s*(.*?)(?=\\n\\s*#|$)', generated_text, re.DOTALL)\n",
    "            if match:\n",
    "                outputs.append(match.group(1).strip())\n",
    "    return outputs if outputs else ['Output not found']\n",
    "\n",
    "def parse_model_output(model_output):\n",
    "    triples = []\n",
    "    lines = [line.strip() for line in model_output.strip().split('\\n') if line.strip()]\n",
    "    pattern = re.compile(r'(.+?)\\s*\\(([^,]+),\\s*([^)]+)\\)')\n",
    "    for line in lines:\n",
    "        for match in pattern.findall(line):\n",
    "            relation, subject, obj = match\n",
    "            triples.append({\n",
    "                \"sub\": subject.strip(),\n",
    "                \"rel\": relation.strip(),\n",
    "                \"obj\": obj.strip()\n",
    "            })\n",
    "    return triples\n",
    "\n",
    "def save_triples(processed_data, output_filepath):\n",
    "    with open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
    "        for entry in processed_data:\n",
    "            json.dump({\"id\": entry[\"id\"], \"triples\": entry[\"triples\"]}, outfile, ensure_ascii=False)\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "def main(jsonl_path, output_path, generator, tokenizer, num_prompts=548, batch_size=16):\n",
    "    prompts = load_prompts(jsonl_path)\n",
    "    results = []\n",
    "\n",
    "    for i in range(0, min(num_prompts, len(prompts)), batch_size):\n",
    "        batch = prompts[i:i + batch_size]\n",
    "        batch_ids = [item['id'] for item in batch]\n",
    "        batch_prompts = [item['prompt'] for item in batch]\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            responses = generate_text(generator, tokenizer, batch_prompts)\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Batch {i}-{i+len(batch)-1} inference time: {elapsed:.2f} seconds\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in batch {i}-{i+batch_size}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # responses length = batch_size * num_return_sequences (2)\n",
    "        # group responses per prompt (2 each)\n",
    "        for idx in range(len(batch)):\n",
    "            # Each prompt has 2 responses: response at idx*2 and idx*2 + 1\n",
    "            prompt_responses = responses[2*idx:2*idx+2]\n",
    "\n",
    "            test_outputs = extract_test_outputs(prompt_responses)\n",
    "\n",
    "            all_triples = []\n",
    "            seen = set()\n",
    "            for test_output in test_outputs:\n",
    "                triples = parse_model_output(test_output)\n",
    "                for triple in triples:\n",
    "                    triple_key = (triple[\"sub\"], triple[\"rel\"], triple[\"obj\"])\n",
    "                    if triple_key not in seen:\n",
    "                        seen.add(triple_key)\n",
    "                        all_triples.append(triple)\n",
    "\n",
    "            print(f\"[{i + idx + 1}/{num_prompts}] ID: {batch_ids[idx]} → Unique triples extracted: {len(all_triples)}\")\n",
    "\n",
    "            results.append({\n",
    "                \"id\": batch_ids[idx],\n",
    "                \"triples\": all_triples\n",
    "            })\n",
    "\n",
    "    save_triples(results, output_path)\n",
    "    print(f\"\\n✅ All {len(results)} prompts processed. Results saved to: {output_path}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed06597a-b5e7-4fc2-9cda-3e80fe08c942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.39it/s]\n",
      "Device set to use cuda:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0-3 inference time: 47.14 seconds\n",
      "[1/1500] ID: ont_10_culture_test_1 → Unique triples extracted: 1\n",
      "[2/1500] ID: ont_10_culture_test_2 → Unique triples extracted: 28\n",
      "[3/1500] ID: ont_10_culture_test_3 → Unique triples extracted: 1\n",
      "[4/1500] ID: ont_10_culture_test_4 → Unique triples extracted: 1\n",
      "Batch 4-7 inference time: 67.92 seconds\n",
      "[5/1500] ID: ont_10_culture_test_5 → Unique triples extracted: 4\n",
      "[6/1500] ID: ont_10_culture_test_6 → Unique triples extracted: 1\n",
      "[7/1500] ID: ont_10_culture_test_7 → Unique triples extracted: 1\n",
      "[8/1500] ID: ont_10_culture_test_8 → Unique triples extracted: 1\n",
      "Batch 8-11 inference time: 68.78 seconds\n",
      "[9/1500] ID: ont_10_culture_test_9 → Unique triples extracted: 5\n",
      "[10/1500] ID: ont_10_culture_test_10 → Unique triples extracted: 1\n",
      "[11/1500] ID: ont_10_culture_test_11 → Unique triples extracted: 1\n",
      "[12/1500] ID: ont_10_culture_test_12 → Unique triples extracted: 4\n",
      "Batch 12-15 inference time: 90.29 seconds\n",
      "[13/1500] ID: ont_10_culture_test_13 → Unique triples extracted: 1\n",
      "[14/1500] ID: ont_10_culture_test_14 → Unique triples extracted: 11\n",
      "[15/1500] ID: ont_10_culture_test_15 → Unique triples extracted: 1\n",
      "[16/1500] ID: ont_10_culture_test_16 → Unique triples extracted: 4\n",
      "Batch 16-19 inference time: 89.43 seconds\n",
      "[17/1500] ID: ont_10_culture_test_17 → Unique triples extracted: 8\n",
      "[18/1500] ID: ont_10_culture_test_18 → Unique triples extracted: 1\n",
      "[19/1500] ID: ont_10_culture_test_19 → Unique triples extracted: 2\n",
      "[20/1500] ID: ont_10_culture_test_20 → Unique triples extracted: 2\n",
      "Batch 20-23 inference time: 45.95 seconds\n",
      "[21/1500] ID: ont_10_culture_test_21 → Unique triples extracted: 1\n",
      "[22/1500] ID: ont_10_culture_test_22 → Unique triples extracted: 1\n",
      "[23/1500] ID: ont_10_culture_test_23 → Unique triples extracted: 1\n",
      "[24/1500] ID: ont_10_culture_test_24 → Unique triples extracted: 2\n",
      "Batch 24-27 inference time: 89.70 seconds\n",
      "[25/1500] ID: ont_10_culture_test_25 → Unique triples extracted: 1\n",
      "[26/1500] ID: ont_10_culture_test_26 → Unique triples extracted: 6\n",
      "[27/1500] ID: ont_10_culture_test_27 → Unique triples extracted: 2\n",
      "[28/1500] ID: ont_10_culture_test_28 → Unique triples extracted: 2\n",
      "Batch 28-31 inference time: 90.13 seconds\n",
      "[29/1500] ID: ont_10_culture_test_29 → Unique triples extracted: 3\n",
      "[30/1500] ID: ont_10_culture_test_30 → Unique triples extracted: 1\n",
      "[31/1500] ID: ont_10_culture_test_31 → Unique triples extracted: 1\n",
      "[32/1500] ID: ont_10_culture_test_32 → Unique triples extracted: 3\n",
      "Batch 32-35 inference time: 89.81 seconds\n",
      "[33/1500] ID: ont_10_culture_test_33 → Unique triples extracted: 3\n",
      "[34/1500] ID: ont_10_culture_test_34 → Unique triples extracted: 15\n",
      "[35/1500] ID: ont_10_culture_test_35 → Unique triples extracted: 1\n",
      "[36/1500] ID: ont_10_culture_test_36 → Unique triples extracted: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 36-39 inference time: 71.80 seconds\n",
      "[37/1500] ID: ont_10_culture_test_37 → Unique triples extracted: 1\n",
      "[38/1500] ID: ont_10_culture_test_38 → Unique triples extracted: 6\n",
      "[39/1500] ID: ont_10_culture_test_39 → Unique triples extracted: 4\n",
      "[40/1500] ID: ont_10_culture_test_40 → Unique triples extracted: 1\n",
      "Batch 40-43 inference time: 91.61 seconds\n",
      "[41/1500] ID: ont_10_culture_test_41 → Unique triples extracted: 10\n",
      "[42/1500] ID: ont_10_culture_test_42 → Unique triples extracted: 1\n",
      "[43/1500] ID: ont_10_culture_test_43 → Unique triples extracted: 5\n",
      "[44/1500] ID: ont_10_culture_test_44 → Unique triples extracted: 1\n",
      "Batch 44-47 inference time: 89.67 seconds\n",
      "[45/1500] ID: ont_10_culture_test_45 → Unique triples extracted: 31\n",
      "[46/1500] ID: ont_10_culture_test_46 → Unique triples extracted: 2\n",
      "[47/1500] ID: ont_10_culture_test_47 → Unique triples extracted: 2\n",
      "[48/1500] ID: ont_10_culture_test_48 → Unique triples extracted: 1\n",
      "Batch 48-51 inference time: 90.01 seconds\n",
      "[49/1500] ID: ont_10_culture_test_49 → Unique triples extracted: 5\n",
      "[50/1500] ID: ont_10_culture_test_50 → Unique triples extracted: 1\n",
      "[51/1500] ID: ont_10_culture_test_51 → Unique triples extracted: 1\n",
      "[52/1500] ID: ont_10_culture_test_52 → Unique triples extracted: 2\n",
      "Batch 52-55 inference time: 46.78 seconds\n",
      "[53/1500] ID: ont_10_culture_test_53 → Unique triples extracted: 2\n",
      "[54/1500] ID: ont_10_culture_test_54 → Unique triples extracted: 1\n",
      "[55/1500] ID: ont_10_culture_test_55 → Unique triples extracted: 21\n",
      "[56/1500] ID: ont_10_culture_test_56 → Unique triples extracted: 1\n",
      "Batch 56-59 inference time: 68.27 seconds\n",
      "[57/1500] ID: ont_10_culture_test_57 → Unique triples extracted: 4\n",
      "[58/1500] ID: ont_10_culture_test_58 → Unique triples extracted: 1\n",
      "[59/1500] ID: ont_10_culture_test_59 → Unique triples extracted: 3\n",
      "[60/1500] ID: ont_10_culture_test_60 → Unique triples extracted: 1\n",
      "Batch 60-63 inference time: 90.25 seconds\n",
      "[61/1500] ID: ont_10_culture_test_61 → Unique triples extracted: 1\n",
      "[62/1500] ID: ont_10_culture_test_62 → Unique triples extracted: 6\n",
      "[63/1500] ID: ont_10_culture_test_63 → Unique triples extracted: 3\n",
      "[64/1500] ID: ont_10_culture_test_64 → Unique triples extracted: 4\n",
      "Batch 64-67 inference time: 46.68 seconds\n",
      "[65/1500] ID: ont_10_culture_test_65 → Unique triples extracted: 1\n",
      "[66/1500] ID: ont_10_culture_test_66 → Unique triples extracted: 1\n",
      "[67/1500] ID: ont_10_culture_test_67 → Unique triples extracted: 2\n",
      "[68/1500] ID: ont_10_culture_test_68 → Unique triples extracted: 1\n",
      "Batch 68-71 inference time: 68.68 seconds\n",
      "[69/1500] ID: ont_10_culture_test_69 → Unique triples extracted: 1\n",
      "[70/1500] ID: ont_10_culture_test_70 → Unique triples extracted: 1\n",
      "[71/1500] ID: ont_10_culture_test_71 → Unique triples extracted: 1\n",
      "[72/1500] ID: ont_10_culture_test_72 → Unique triples extracted: 5\n",
      "Batch 72-75 inference time: 89.79 seconds\n",
      "[73/1500] ID: ont_10_culture_test_73 → Unique triples extracted: 6\n",
      "[74/1500] ID: ont_10_culture_test_74 → Unique triples extracted: 1\n",
      "[75/1500] ID: ont_10_culture_test_75 → Unique triples extracted: 7\n",
      "[76/1500] ID: ont_10_culture_test_76 → Unique triples extracted: 1\n",
      "Batch 76-79 inference time: 67.97 seconds\n",
      "[77/1500] ID: ont_10_culture_test_77 → Unique triples extracted: 2\n",
      "[78/1500] ID: ont_10_culture_test_78 → Unique triples extracted: 4\n",
      "[79/1500] ID: ont_10_culture_test_79 → Unique triples extracted: 1\n",
      "[80/1500] ID: ont_10_culture_test_80 → Unique triples extracted: 21\n",
      "Batch 80-83 inference time: 68.02 seconds\n",
      "[81/1500] ID: ont_10_culture_test_81 → Unique triples extracted: 1\n",
      "[82/1500] ID: ont_10_culture_test_82 → Unique triples extracted: 29\n",
      "[83/1500] ID: ont_10_culture_test_83 → Unique triples extracted: 3\n",
      "[84/1500] ID: ont_10_culture_test_84 → Unique triples extracted: 1\n",
      "Batch 84-87 inference time: 68.52 seconds\n",
      "[85/1500] ID: ont_10_culture_test_85 → Unique triples extracted: 8\n",
      "[86/1500] ID: ont_10_culture_test_86 → Unique triples extracted: 1\n",
      "[87/1500] ID: ont_10_culture_test_87 → Unique triples extracted: 1\n",
      "[88/1500] ID: ont_10_culture_test_88 → Unique triples extracted: 8\n",
      "Batch 88-91 inference time: 90.08 seconds\n",
      "[89/1500] ID: ont_10_culture_test_89 → Unique triples extracted: 1\n",
      "[90/1500] ID: ont_10_culture_test_90 → Unique triples extracted: 7\n",
      "[91/1500] ID: ont_10_culture_test_91 → Unique triples extracted: 2\n",
      "[92/1500] ID: ont_10_culture_test_92 → Unique triples extracted: 3\n",
      "Batch 92-95 inference time: 68.01 seconds\n",
      "[93/1500] ID: ont_10_culture_test_93 → Unique triples extracted: 1\n",
      "[94/1500] ID: ont_10_culture_test_94 → Unique triples extracted: 1\n",
      "[95/1500] ID: ont_10_culture_test_95 → Unique triples extracted: 2\n",
      "[96/1500] ID: ont_10_culture_test_96 → Unique triples extracted: 1\n",
      "Batch 96-99 inference time: 68.72 seconds\n",
      "[97/1500] ID: ont_10_culture_test_97 → Unique triples extracted: 1\n",
      "[98/1500] ID: ont_10_culture_test_98 → Unique triples extracted: 2\n",
      "[99/1500] ID: ont_10_culture_test_99 → Unique triples extracted: 4\n",
      "[100/1500] ID: ont_10_culture_test_100 → Unique triples extracted: 1\n",
      "Batch 100-103 inference time: 69.36 seconds\n",
      "[101/1500] ID: ont_10_culture_test_101 → Unique triples extracted: 2\n",
      "[102/1500] ID: ont_10_culture_test_102 → Unique triples extracted: 2\n",
      "[103/1500] ID: ont_10_culture_test_103 → Unique triples extracted: 2\n",
      "[104/1500] ID: ont_10_culture_test_104 → Unique triples extracted: 2\n",
      "Batch 104-107 inference time: 89.85 seconds\n",
      "[105/1500] ID: ont_10_culture_test_105 → Unique triples extracted: 1\n",
      "[106/1500] ID: ont_10_culture_test_106 → Unique triples extracted: 1\n",
      "[107/1500] ID: ont_10_culture_test_107 → Unique triples extracted: 2\n",
      "[108/1500] ID: ont_10_culture_test_108 → Unique triples extracted: 1\n",
      "Batch 108-111 inference time: 67.65 seconds\n",
      "[109/1500] ID: ont_10_culture_test_109 → Unique triples extracted: 1\n",
      "[110/1500] ID: ont_10_culture_test_110 → Unique triples extracted: 3\n",
      "[111/1500] ID: ont_10_culture_test_111 → Unique triples extracted: 1\n",
      "[112/1500] ID: ont_10_culture_test_112 → Unique triples extracted: 1\n",
      "Batch 112-115 inference time: 89.29 seconds\n",
      "[113/1500] ID: ont_10_culture_test_113 → Unique triples extracted: 2\n",
      "[114/1500] ID: ont_10_culture_test_114 → Unique triples extracted: 1\n",
      "[115/1500] ID: ont_10_culture_test_115 → Unique triples extracted: 1\n",
      "[116/1500] ID: ont_10_culture_test_116 → Unique triples extracted: 1\n",
      "Batch 116-119 inference time: 89.53 seconds\n",
      "[117/1500] ID: ont_10_culture_test_117 → Unique triples extracted: 6\n",
      "[118/1500] ID: ont_10_culture_test_118 → Unique triples extracted: 3\n",
      "[119/1500] ID: ont_10_culture_test_119 → Unique triples extracted: 3\n",
      "[120/1500] ID: ont_10_culture_test_120 → Unique triples extracted: 1\n",
      "Batch 120-123 inference time: 89.53 seconds\n",
      "[121/1500] ID: ont_10_culture_test_121 → Unique triples extracted: 10\n",
      "[122/1500] ID: ont_10_culture_test_122 → Unique triples extracted: 2\n",
      "[123/1500] ID: ont_10_culture_test_123 → Unique triples extracted: 1\n",
      "[124/1500] ID: ont_10_culture_test_124 → Unique triples extracted: 1\n",
      "Batch 124-127 inference time: 89.96 seconds\n",
      "[125/1500] ID: ont_10_culture_test_125 → Unique triples extracted: 2\n",
      "[126/1500] ID: ont_10_culture_test_126 → Unique triples extracted: 3\n",
      "[127/1500] ID: ont_10_culture_test_127 → Unique triples extracted: 1\n",
      "[128/1500] ID: ont_10_culture_test_128 → Unique triples extracted: 4\n",
      "Batch 128-131 inference time: 67.36 seconds\n",
      "[129/1500] ID: ont_10_culture_test_129 → Unique triples extracted: 1\n",
      "[130/1500] ID: ont_10_culture_test_130 → Unique triples extracted: 2\n",
      "[131/1500] ID: ont_10_culture_test_131 → Unique triples extracted: 1\n",
      "[132/1500] ID: ont_10_culture_test_132 → Unique triples extracted: 2\n",
      "Batch 132-135 inference time: 67.04 seconds\n",
      "[133/1500] ID: ont_10_culture_test_133 → Unique triples extracted: 2\n",
      "[134/1500] ID: ont_10_culture_test_134 → Unique triples extracted: 2\n",
      "[135/1500] ID: ont_10_culture_test_135 → Unique triples extracted: 1\n",
      "[136/1500] ID: ont_10_culture_test_136 → Unique triples extracted: 2\n",
      "Batch 136-139 inference time: 88.68 seconds\n",
      "[137/1500] ID: ont_10_culture_test_137 → Unique triples extracted: 1\n",
      "[138/1500] ID: ont_10_culture_test_138 → Unique triples extracted: 2\n",
      "[139/1500] ID: ont_10_culture_test_139 → Unique triples extracted: 1\n",
      "[140/1500] ID: ont_10_culture_test_140 → Unique triples extracted: 2\n",
      "Batch 140-143 inference time: 67.31 seconds\n",
      "[141/1500] ID: ont_10_culture_test_141 → Unique triples extracted: 1\n",
      "[142/1500] ID: ont_10_culture_test_142 → Unique triples extracted: 1\n",
      "[143/1500] ID: ont_10_culture_test_143 → Unique triples extracted: 2\n",
      "[144/1500] ID: ont_10_culture_test_144 → Unique triples extracted: 4\n",
      "Batch 144-147 inference time: 67.41 seconds\n",
      "[145/1500] ID: ont_10_culture_test_145 → Unique triples extracted: 3\n",
      "[146/1500] ID: ont_10_culture_test_146 → Unique triples extracted: 1\n",
      "[147/1500] ID: ont_10_culture_test_147 → Unique triples extracted: 1\n",
      "[148/1500] ID: ont_10_culture_test_148 → Unique triples extracted: 1\n",
      "Batch 148-151 inference time: 67.69 seconds\n",
      "[149/1500] ID: ont_10_culture_test_149 → Unique triples extracted: 1\n",
      "[150/1500] ID: ont_10_culture_test_150 → Unique triples extracted: 1\n",
      "[151/1500] ID: ont_10_culture_test_151 → Unique triples extracted: 3\n",
      "[152/1500] ID: ont_10_culture_test_152 → Unique triples extracted: 1\n",
      "Batch 152-155 inference time: 68.50 seconds\n",
      "[153/1500] ID: ont_10_culture_test_153 → Unique triples extracted: 1\n",
      "[154/1500] ID: ont_10_culture_test_154 → Unique triples extracted: 1\n",
      "[155/1500] ID: ont_10_culture_test_155 → Unique triples extracted: 2\n",
      "[156/1500] ID: ont_10_culture_test_156 → Unique triples extracted: 2\n",
      "Batch 156-158 inference time: 66.66 seconds\n",
      "[157/1500] ID: ont_10_culture_test_157 → Unique triples extracted: 15\n",
      "[158/1500] ID: ont_10_culture_test_158 → Unique triples extracted: 7\n",
      "[159/1500] ID: ont_10_culture_test_159 → Unique triples extracted: 9\n",
      "\n",
      "✅ All 159 prompts processed. Results saved to: /upb/users/b/balram/profiles/unix/cs/Text2KG/withont/data/wikidata/response_run4/Llama3/cot_response_without_quant_batch/ont_10_culture_llm_response_improved.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = '/upb/users/b/balram/profiles/unix/cs/Text2KG/withont/data/wikidata/input_prompts/cot_prompts/ont_10_culture_prompts_improved.jsonl'\n",
    "    output_file = \"/upb/users/b/balram/profiles/unix/cs/Text2KG/withont/data/wikidata/response_run4/Llama3/cot_response_without_quant_batch/ont_10_culture_llm_response_improved.jsonl\"\n",
    "    text_pipe, tokenizer = setup_model(\"meta-llama/Meta-Llama-3-8B\")\n",
    "    main(input_file, output_file, text_pipe, tokenizer, num_prompts=1500, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a6e75-bdea-49e9-88cf-e4b59bf3f5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e2d44-032c-43c6-8ef0-d446478a14fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46206e18-fb88-4898-ad51-e17d6e6f90c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f6b86-aaf4-4301-9729-b31cdc60540c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e21bf3-a419-4cf3-b0de-3e51528508bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70373f0-06d0-46c3-acbf-3f8b112276e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KG Pipeline (GPU)",
   "language": "python",
   "name": "kg_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
