{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bec137-7d61-4953-9780-c61c5108d9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f641d19-b48e-4835-9c0f-e39fa3610d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a12b392b-8f52-479d-a1ef-e2a91b5e049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /upb/users/b/balram/profiles/unix/cs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /upb/users/b/balram/profiles/unix/cs/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from typing import Dict, List\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# === Utility Normalization Methods === #\n",
    "\n",
    "def lemmatize_and_normalize(text, lemmatizer):\n",
    "    \"\"\"Lowercase, replace underscores, remove all non-alphanumerics except space,\n",
    "    then tokenize, lemmatize, and join without spaces.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"_\", \" \")              # underscores → spaces\n",
    "    text = re.sub(r\"[^a-z0-9 ]\", \"\", text)    # remove special chars except space\n",
    "    text = text.strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized = \"\".join(lemmatizer.lemmatize(token) for token in tokens)\n",
    "    return lemmatized\n",
    "\n",
    "def normalize_triple(sub_label, rel_label, obj_label, lemmatizer, stem_rel=False):\n",
    "    \"\"\"Normalize triple components consistently.\"\"\"\n",
    "    sub_label = lemmatize_and_normalize(sub_label, lemmatizer)\n",
    "    obj_label = lemmatize_and_normalize(obj_label, lemmatizer)\n",
    "\n",
    "    rel_label_clean = rel_label.lower().replace(\"_\", \" \")\n",
    "    rel_label_clean = re.sub(r\"[^a-z0-9 ]\", \"\", rel_label_clean).strip()\n",
    "    if stem_rel:\n",
    "        rel_label_clean = \" \".join([lemmatizer.lemmatize(w) for w in word_tokenize(rel_label_clean)])\n",
    "    rel_label_clean = re.sub(r\"\\s+\", \"\", rel_label_clean)\n",
    "\n",
    "    return f\"{sub_label}{rel_label_clean}{obj_label}\"\n",
    "\n",
    "# === Core Evaluation Metrics === #\n",
    "\n",
    "def calculate_precision_recall_f1(gold_set, pred_set):\n",
    "    #print(\"gold_set\", gold_set)\n",
    "    #print(\"pred_set\", pred_set)\n",
    "    if not pred_set:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    intersection = gold_set.intersection(pred_set)\n",
    "    p = len(intersection) / len(pred_set)\n",
    "    r = len(intersection) / len(gold_set)\n",
    "    f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "    return p, r, f1\n",
    "\n",
    "def get_subject_object_hallucinations(lemmatizer, ontology, sentence, triples):\n",
    "    if not triples:\n",
    "        return 0, 0\n",
    "    extended_sentence = sentence + \" \" + \" \".join([c[\"label\"] for c in ontology['concepts']])\n",
    "    normalized_sentence = lemmatize_and_normalize(extended_sentence, lemmatizer)\n",
    "\n",
    "    subj_halluc, obj_halluc = 0, 0\n",
    "    for sub, rel, obj in triples:\n",
    "        norm_sub = lemmatize_and_normalize(sub, lemmatizer)\n",
    "        norm_obj = lemmatize_and_normalize(obj, lemmatizer)\n",
    "        if norm_sub not in normalized_sentence:\n",
    "            subj_halluc += 1\n",
    "        if norm_obj not in normalized_sentence:\n",
    "            obj_halluc += 1\n",
    "\n",
    "    return subj_halluc / len(triples), obj_halluc / len(triples)\n",
    "\n",
    "def get_ontology_conformance(ontology, triples):\n",
    "    if not triples:\n",
    "        return 1, 0\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ont_rels = {lemmatize_and_normalize(rel['label'], lemmatizer) for rel in ontology['relations']}\n",
    "    num_conformant = sum(\n",
    "        1 for tr in triples if lemmatize_and_normalize(tr[1], lemmatizer) in ont_rels\n",
    "    )\n",
    "    #print(\"len(triples)\",len(triples))\n",
    "    conformance = num_conformant / len(triples)\n",
    "    return conformance, 1 - conformance\n",
    "\n",
    "# === Main Evaluation Pipeline === #\n",
    "\n",
    "def evaluate_and_save_results(ground_truth_data, ontology, model_data, output_file):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    results = []\n",
    "\n",
    "    for gt_entry, model_entry in zip(ground_truth_data, model_data):\n",
    "        if not gt_entry.get('triples'):\n",
    "            continue\n",
    "\n",
    "        gt_triples = [[tr['sub'], tr['rel'], tr['obj']] for tr in gt_entry['triples']]\n",
    "        #print(model_entry['triples'])\n",
    "        system_triples = [[tr['sub'], tr['rel'], tr['obj']] for tr in model_entry['triples']]\n",
    "        #system_triples = model_entry['triples']\n",
    "        #print(\"gt_triples\", gt_triples)\n",
    "        #print(\"system_triples\", system_triples)\n",
    "\n",
    "        # Filter system triples by GT relations (normalized)\n",
    "        gt_relations = {lemmatize_and_normalize(tr[1], lemmatizer) for tr in gt_triples}\n",
    "        filtered_system_triples = [tr for tr in system_triples if lemmatize_and_normalize(tr[1], lemmatizer) in gt_relations]\n",
    "\n",
    "        normalized_gt_triples = {normalize_triple(tr[0], tr[1], tr[2], lemmatizer) for tr in gt_triples}\n",
    "        normalized_system_triples = {normalize_triple(tr[0], tr[1], tr[2], lemmatizer) for tr in filtered_system_triples}\n",
    "\n",
    "        #print(\"normalized_gt_triples\", normalized_gt_triples)\n",
    "        #print(\"normalized_system_triples\", normalized_system_triples)\n",
    "\n",
    "        precision, recall, f1 = calculate_precision_recall_f1(normalized_gt_triples, normalized_system_triples)\n",
    "        ont_conformance, rel_hallucination = get_ontology_conformance(ontology, system_triples)\n",
    "        subj_hallucination, obj_hallucination = get_subject_object_hallucinations(lemmatizer, ontology, gt_entry['sent'], system_triples)\n",
    "\n",
    "        result = {\n",
    "            \"id\": gt_entry['id'],\n",
    "            \"precision\": f\"{precision:.2f}\",\n",
    "            \"recall\": f\"{recall:.2f}\",\n",
    "            \"f1\": f\"{f1:.2f}\",\n",
    "            \"onto_conf\": f\"{ont_conformance:.2f}\",\n",
    "            \"rel_halluc\": f\"{rel_hallucination:.2f}\",\n",
    "            \"sub_halluc\": f\"{subj_hallucination:.2f}\",\n",
    "            \"obj_halluc\": f\"{obj_hallucination:.2f}\",\n",
    "            \"llm_triples\": system_triples,\n",
    "            \"filtered_llm_triples\": filtered_system_triples,\n",
    "            \"gt_triples\": gt_triples,\n",
    "            \"sent\": gt_entry['sent']\n",
    "        }\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for res in results:\n",
    "            f.write(json.dumps(res) + \"\\n\")\n",
    "\n",
    "# === File Reading Utilities === #\n",
    "\n",
    "def read_jsonl(file_path, required_keys=None):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "        if required_keys:\n",
    "            for entry in data:\n",
    "                if not all(key in entry for key in required_keys):\n",
    "                    raise ValueError(f\"Missing keys in entry: {entry}\")\n",
    "        return data\n",
    "\n",
    "def read_ontology_json(json_path):\n",
    "    with open(json_path) as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c86c2b-c04b-4d36-92db-51321616b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_evaluations_for_all_categories()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec5865-4211-4ed6-add7-94e3830cb2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "54d23f5b-4f97-4114-9faf-e28b1e4efc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluations_for_all_categories_wikidata():\n",
    "    categories = [\n",
    "        \"movie\", \"music\", \"sport\", \"book\", \"military\",\n",
    "        \"computer\", \"space\", \"politics\", \"nature\", \"culture\"\n",
    "    ]\n",
    "\n",
    "    for i, category in enumerate(categories, start=1):\n",
    "        print(f\"\\n=== Running Evaluation for Category: {i} - {category} ===\")\n",
    "\n",
    "        output_filepath = f\"../data/wikidata/improvised_evaluation_statistics/Llama/without_missing_GT/run/improved_evaluation/ont_{i}_{category}_llm_stats_improved.jsonl\"\n",
    "        ground_truth_filepath = f\"../data/wikidata/ground_truth/ont_{i}_{category}_ground_truth.jsonl\"\n",
    "        ontology_filepath = f\"/upb/users/b/balram/profiles/unix/cs/Text2KG/withont/data/wikidata/ontology/{i}_{category}_ontology.json\"\n",
    "        model_response_filepath = f\"/upb/users/b/balram/profiles/unix/cs/Text2KG/withont/data/wikidata/response_run2/Llama3/cot_response_without_quant_batch/ont_{i}_{category}_llm_response_improved.jsonl\"\n",
    "\n",
    "        try:\n",
    "            # Load Data\n",
    "            ground_truth_data = read_jsonl(ground_truth_filepath)\n",
    "            ontology_data = read_ontology_json(ontology_filepath)\n",
    "            model_data = read_jsonl(model_response_filepath, required_keys=['id', 'triples'])\n",
    "\n",
    "            # Run Evaluation\n",
    "            evaluate_and_save_results(ground_truth_data, ontology_data, model_data, output_filepath)\n",
    "            print(f\"✅ Successfully evaluated and saved results for {category}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing category '{category}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5a7604f0-35e7-4f36-9e26-6f942fb0a494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Evaluation for Category: 1 - movie ===\n",
      "✅ Successfully evaluated and saved results for movie\n",
      "\n",
      "=== Running Evaluation for Category: 2 - music ===\n",
      "✅ Successfully evaluated and saved results for music\n",
      "\n",
      "=== Running Evaluation for Category: 3 - sport ===\n",
      "✅ Successfully evaluated and saved results for sport\n",
      "\n",
      "=== Running Evaluation for Category: 4 - book ===\n",
      "✅ Successfully evaluated and saved results for book\n",
      "\n",
      "=== Running Evaluation for Category: 5 - military ===\n",
      "✅ Successfully evaluated and saved results for military\n",
      "\n",
      "=== Running Evaluation for Category: 6 - computer ===\n",
      "✅ Successfully evaluated and saved results for computer\n",
      "\n",
      "=== Running Evaluation for Category: 7 - space ===\n",
      "✅ Successfully evaluated and saved results for space\n",
      "\n",
      "=== Running Evaluation for Category: 8 - politics ===\n",
      "✅ Successfully evaluated and saved results for politics\n",
      "\n",
      "=== Running Evaluation for Category: 9 - nature ===\n",
      "✅ Successfully evaluated and saved results for nature\n",
      "\n",
      "=== Running Evaluation for Category: 10 - culture ===\n",
      "✅ Successfully evaluated and saved results for culture\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_evaluations_for_all_categories_wikidata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca497015-1042-45ef-824b-7fd3f6809631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83d256e-613f-4453-85f0-50c0b1a08d32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
