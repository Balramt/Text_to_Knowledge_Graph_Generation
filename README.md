# Leveraging Large Language Models for KG Construction

This repository is part of the research project on **Leveraging Large Language Models for KG Construction** using Large Language Models (LLMs). The benchmark aims to evaluate LLMs for extracting knowledge graph triples from natural language while adhering to a given ontology. The focus is on key evaluation metrics such as precision, recall, F1-score, ontology conformance, and hallucination rates.

## ğŸ§ª Example

**ğŸ” Test Sentence Example:**

```json
{
  "id": "ont_film_test_1",
  "sent":"Super Capers is a 98 minute English movie that was distributed by Roadside Attractions and Lionsgate. It was directed by Ray Griggs and edited by Stacy Katzman.The budget was $2,000,000."
}
```

**ğŸ¼ Ontology Used:** [Film Ontology](data/dbpedia/ontology/19_film_ontology.json)

**ğŸ“ˆ Output Graph:**

![Graph Example](UI/KG_Visualization.PNG)
---
## ğŸ§  `src/` â€“ Main Source Directory
The `src` directory contains all the code for running LLMs, generating RDF triples, and evaluating outputs. It is organized into submodules for evaluation and model generation.
---
### ğŸ“‚ [`evaluation`](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/src/evaluation)
Scripts to evaluate the triples extracted from LLM responses.
* ğŸ“˜ **[Baseline Evaluation](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/blob/main/src/evaluation/Baseline_evaluation.ipynb)**
  Evaluates precision, recall, F1, and hallucinations (ontology, subject, object, relation) using raw LLM outputs.
* ğŸ› ï¸ **[Improvised Evaluation](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/blob/main/src/evaluation/Evaluation_improvised.ipynb)**
  More robust evaluation that cleans special characters and symbols for accurate matching.
---
### ğŸ¤– [`llm_models`](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/src/llm_models)
LLM-based triple generation modules.
* ğŸ”¹ **[LLaMA](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/blob/main/src/llm_models/Llama3_with_batch_without_quant.ipynb)**
  Uses LLaMA to generate text and extract triples from structured prompts.
* ğŸ”¹ **[Mistral](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/blob/main/src/llm_models/Mistral_Batch.ipynb)**
  Parallel implementation using Mistral model to generate and parse triples from prompts.
---

## ğŸ“¦ Data Structure Overview

### ğŸ—‚ï¸ `wikidata_tekgen`
* [Wikidata-TekGen](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/wikidata)
* **Ontologies (10)**: [ontologies](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/wikidata/ontology) â€“ Ontology files used for triple validation.
* **Ground Truth**: [ground\_truth](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/wikidata/ground_truth) â€“ Gold standard triples for evaluation.
* **Prompts**: [prompts](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/wikidata/prompts) â€“ Natural language prompts generated for test samples.
  
#### ğŸ“Š Baselines â€“ Evaluation Results and LLM Responses
##### ğŸ”¹ [Alpaca-LoRA-13B](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/wikidata/baselines/Alpaca-LoRA-13B)
* [llm\_responses](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/wikidata/baselines/Alpaca-LoRA-13B/llm_response) â€“ Raw responses + extracted triples
* [eval\_metrics](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/wikidata/baselines/Alpaca-LoRA-13B/evaluation_statistics/baseline_statistics) â€“ Ontology-level + aggregated results
##### ğŸ”¹ [Vicuna-13B](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/wikidata/baselines/Vicuna-13B)
* [llm\_responses](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/wikidata/baselines/Vicuna-13B/llm_response) â€“ Raw responses + extracted triples
* [eval\_metrics](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/wikidata/baselines/Vicuna-13B/evaluation_statistics/baseline_statistics) â€“ Ontology-level + aggregated results
##### ğŸ”¹ [Llama-8B](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/wikidata/baselines/Llama-8B)
* [llm\_responses](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/wikidata/baselines/Llama-8B/llm_response) â€“ Raw responses + extracted triples
* [eval\_metrics](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/wikidata/baselines/Llama-8B/evaluation_statistics) â€“ Ontology-level + aggregated results
##### ğŸ”¹ [Mistral-7B](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/wikidata/baselines/Mistral-7B)
* [llm\_responses](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/wikidata/baselines/Mistral-7B/llm_response) â€“ Raw responses + extracted triples
* [eval\_metrics](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/wikidata/baselines/Mistral-7B/evaluation_statistics) â€“ Ontology-level + aggregated results


### ğŸ—‚ï¸ `dbpedia_webnlg`
* [DBpedia-Webnlg](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/dbpedia)
* **Ontologies (19)**: [ontologies](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/dbpedia/ontology) â€“ DBpedia ontologies for triple evaluation.
* **Ground Truth**: [ground\_truth](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/dbpedia/ground_truth) â€“ Gold standard triple data.
* **Prompts**: [prompts](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/dbpedia/prompts) â€“ Prompt files for LLM-based triple generation.

#### ğŸ“Š Baselines â€“ Evaluation Results and LLM Responses
##### ğŸ”¹ [Alpaca-LoRA-13B](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/dbpedia/baselines/Alpaca-LoRA-13B)
* [llm\_responses](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/dbpedia/baselines/Alpaca-LoRA-13B/llm_response) â€“ Raw responses + extracted triples
* [eval\_metrics](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/dbpedia/baselines/Alpaca-LoRA-13B/evaluation_statistics/baseline_statistics) â€“ Ontology-level + aggregated results
##### ğŸ”¹ [Vicuna-13B](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/dbpedia/baselines/Vicuna-13B)
* [llm\_responses](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/dbpedia/baselines/Vicuna-13B/llm_response) â€“ Raw responses + extracted triples
* [eval\_metrics](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/dbpedia/baselines/Vicuna-13B/evaluation_statistics/baseline_statistics) â€“ Ontology-level + aggregated results
##### ğŸ”¹ [Llama-8B](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/dbpedia/baselines/Llama-8B)
* [llm\_responses](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/dbpedia/baselines/Llama-8B/llm_response) â€“ Raw responses + extracted triples
* [eval\_metrics](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/dbpedia/baselines/Llama-8B/evaluation_statistics) â€“ Ontology-level + aggregated results
##### ğŸ”¹ [Mistral-7B](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/dbpedia/baselines/Mistral-7B)
* [llm\_responses](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/dbpedia/baselines/Mistral-7B/llm_response) â€“ Raw responses + extracted triples
* [eval\_metrics](https://github.com/Balramt/Text_to_Knowledge_Graph_Generation/tree/main/data/dbpedia/baselines/Mistral-7B/evaluation_statistics) â€“ Ontology-level + aggregated results

## ğŸ“Š Wilcoxon Signed-Rank Test: LLM Comparison on Wikidata and DBpedia

We conducted pairwise statistical significance tests using the **Wilcoxon Signed-Rank Test** to compare the performance of large language models (LLMs) â€” **LLama 3-8B**, **Mistral**, **Alpaca-LoRA-13B**, and **Vicuna** â€” on two datasets: **Wikidata** and **DBpedia**. 

Two types of evaluations were used:
- **Improvised Evaluation**: Sample-wise F1 scores
- **Baseline Evaluation**: Per-domain average F1 scores

---

### ğŸ“ Dataset: [Wikidata](#)
We can clearly see that **LLama 3-8B** and **Mistral** consistently outperform the baseline models **Alpaca-LoRA-13B** and **Vicuna** across both evaluation methods.

#### ğŸ” Wilcoxon Signed-Rank Test on Improvised Evaluation (Wikidata)


---
## ğŸ“ Folder Tree (Example)
```
.
â”œâ”€â”€ UI/
â”‚   â””â”€â”€ KG_Visualization.PNG
â”‚   â”œâ”€â”€ Tesct2KG_UI.ipynb
|   â”œâ”€â”€ UI_Readme.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â””â”€â”€ Baseline_evaluation.ipynb
â”‚   â”‚   â””â”€â”€ Evaluation_improvised.ipynb
â”‚   â””â”€â”€ llm_models/
â”‚       â””â”€â”€ Llama3_with_batch_without_quant.ipynb
â”‚       â””â”€â”€ Mistral_Batch.ipynb
â”‚
â”œâ”€â”€ evaluation_table/
|   â”œâ”€â”€dbpedia_ebaluation_result/
|   |  â””â”€â”€ Evaluation_Table_Improved.md
|   |  â””â”€â”€ Evaluation_Table_baseline.md
|   |  â””â”€â”€ wilcoxon_rank_test_dbpedia.ipynb
|   â”œâ”€â”€dbpedia_ebaluation_result/
|      â””â”€â”€ Evaluation_Table_Improved.md
|      â””â”€â”€ Evaluation_Table_baseline.md
|      â””â”€â”€ wilcoxon_rank_test_wikidata.ipynb  
|
â””â”€â”€ data/
    â”œâ”€â”€ dbpedia/
    â”‚   â”œâ”€â”€ ontology/
    â”‚   â”œâ”€â”€ ground_truth/
    â”‚   â”œâ”€â”€ prompts/
    â”‚   â””â”€â”€ baselines/
    â”‚       â”œâ”€â”€ Alpaca-LoRA-13B/
    â”‚       â”‚   â”œâ”€â”€ llm_response/
    â”‚       â”‚   â””â”€â”€ evaluation_statistics/
    â”‚       â”‚       â””â”€â”€ baseline_statistics/
    â”‚       â”œâ”€â”€ Vicuna-13B/
    â”‚       â”‚   â”œâ”€â”€ llm_response/
    â”‚       â”‚   â””â”€â”€ evaluation_statistics/
    â”‚       â”‚       â””â”€â”€ baseline_statistics/
    â”‚       â”œâ”€â”€ Llama-8B/
    â”‚       â”‚   â”œâ”€â”€ llm_response/
    â”‚       â”‚   â””â”€â”€ evaluation_statistics/
    |       |   â””â”€â”€ evaluation_statistics/
    â”‚       â””â”€â”€ Mistral-7B/
    â”‚           â”œâ”€â”€ llm_response/
    â”‚           â””â”€â”€ evaluation_statistics/
    |           â””â”€â”€ evaluation_statistics/
    â”‚
    â””â”€â”€ wikidata/
        â”œâ”€â”€ ontology/
        â”œâ”€â”€ ground_truth/
        â”œâ”€â”€ prompts/
        â””â”€â”€ baselines/
            â”œâ”€â”€ Alpaca-LoRA-13B/
            â”‚   â”œâ”€â”€ llm_response/
            â”‚   â””â”€â”€ evaluation_statistics/
            â”‚       â””â”€â”€ baseline_statistics/
            â”œâ”€â”€ Vicuna-13B/
            â”‚   â”œâ”€â”€ llm_response/
            â”‚   â””â”€â”€ evaluation_statistics/
            â”‚       â””â”€â”€ baseline_statistics/
            â”œâ”€â”€ Llama-8B/
            â”‚   â”œâ”€â”€ llm_response/
            â”‚   â””â”€â”€ evaluation_statistics/
            â””â”€â”€ Mistral-7B/
                â”œâ”€â”€ llm_response/
                â””â”€â”€ evaluation_statistics/

```
