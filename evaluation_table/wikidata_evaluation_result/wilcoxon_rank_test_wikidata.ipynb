{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d56b274-2a41-4bd5-a68a-a0853d26583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "279e36a4-b00b-434a-a98d-1965d0da1490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Wilcoxon Signed-Rank Test on Improvised Evaluation (Wikidata) ===\n",
      "\n",
      "[Wikidata] LLama 3-8B vs Alpaca-LoRA-13B\n",
      "Wilcoxon statistic = 0.0000, p-value = 0.0020\n",
      "→ LLama 3-8B significantly outperforms Alpaca-LoRA-13B (p < 0.05)\n",
      "\n",
      "[Wikidata] LLama 3-8B vs Vicuna\n",
      "Wilcoxon statistic = 0.0000, p-value = 0.0020\n",
      "→ LLama 3-8B significantly outperforms Vicuna (p < 0.05)\n",
      "\n",
      "[Wikidata] Mistral vs Alpaca-LoRA-13B\n",
      "Wilcoxon statistic = 1.0000, p-value = 0.0039\n",
      "→ Mistral significantly outperforms Alpaca-LoRA-13B (p < 0.05)\n",
      "\n",
      "[Wikidata] Mistral vs Vicuna\n",
      "Wilcoxon statistic = 6.0000, p-value = 0.0254\n",
      "→ Mistral significantly outperforms Vicuna (p < 0.05)\n",
      "\n",
      "=== Wilcoxon Signed-Rank Test on Baseline Evaluation (avg_f1 across domains) ===\n",
      "\n",
      "[Baseline] LLama 3-8B vs Alpaca-LoRA-13B\n",
      "Wilcoxon statistic = 0.0000, p-value = 0.0020\n",
      "→ LLama 3-8B significantly outperforms Alpaca-LoRA-13B (p < 0.05)\n",
      "\n",
      "[Baseline] LLama 3-8B vs Vicuna\n",
      "Wilcoxon statistic = 0.0000, p-value = 0.0039\n",
      "→ LLama 3-8B significantly outperforms Vicuna (p < 0.05)\n",
      "\n",
      "[Baseline] Mistral vs Alpaca-LoRA-13B\n",
      "Wilcoxon statistic = 0.0000, p-value = 0.0020\n",
      "→ Mistral significantly outperforms Alpaca-LoRA-13B (p < 0.05)\n",
      "\n",
      "[Baseline] Mistral vs Vicuna\n",
      "Wilcoxon statistic = 6.0000, p-value = 0.0273\n",
      "→ Mistral significantly outperforms Vicuna (p < 0.05)\n"
     ]
    }
   ],
   "source": [
    "# === Improvised Evaluation (Sample-wise F1 Scores for Wikidata) ===\n",
    "llama3_f1_wikidata = [0.32, 0.36, 0.53, 0.35, 0.41, 0.40, 0.74, 0.51, 0.49, 0.58]\n",
    "mistral_f1_wikidata = [0.30, 0.37, 0.44, 0.33, 0.40, 0.36, 0.72, 0.44, 0.41, 0.53]\n",
    "alpaca_f1_wikidata = [0.17, 0.22, 0.45, 0.19, 0.21, 0.29, 0.55, 0.21, 0.27, 0.15]\n",
    "vicuna_f1_wikidata = [0.25, 0.32, 0.52, 0.26, 0.24, 0.35, 0.66, 0.33, 0.25, 0.31]\n",
    "\n",
    "# === Baseline Evaluation (avg_f1 Scores Per Domain) ===\n",
    "llama3_f1_baseline = [0.27, 0.32, 0.60, 0.34, 0.37, 0.37, 0.70, 0.53, 0.47, 0.63]\n",
    "mistral_f1_baseline = [0.29, 0.34, 0.47, 0.32, 0.40, 0.33, 0.70, 0.40, 0.39, 0.45]\n",
    "alpaca_f1_baseline = [0.17, 0.22, 0.45, 0.19, 0.21, 0.29, 0.55, 0.21, 0.27, 0.15]\n",
    "vicuna_f1_baseline = [0.25, 0.32, 0.52, 0.26, 0.24, 0.35, 0.66, 0.33, 0.25, 0.31]\n",
    "\n",
    "# === Wilcoxon Test Function ===\n",
    "def run_wilcoxon(model1_scores, model2_scores, name1, name2, dataset):\n",
    "    stat, p = wilcoxon(model1_scores, model2_scores)\n",
    "    print(f\"\\n[{dataset}] {name1} vs {name2}\")\n",
    "    print(f\"Wilcoxon statistic = {stat:.4f}, p-value = {p:.4f}\")\n",
    "    if p < 0.05:\n",
    "        print(f\"→ {name1} significantly outperforms {name2} (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"→ No significant difference between {name1} and {name2} (p ≥ 0.05)\")\n",
    "\n",
    "# === Run Comparisons ===\n",
    "print(\"=== Wilcoxon Signed-Rank Test on Improvised Evaluation (Wikidata) ===\")\n",
    "run_wilcoxon(llama3_f1_wikidata, alpaca_f1_wikidata, \"LLama 3-8B\", \"Alpaca-LoRA-13B\", \"Wikidata\")\n",
    "run_wilcoxon(llama3_f1_wikidata, vicuna_f1_wikidata, \"LLama 3-8B\", \"Vicuna\", \"Wikidata\")\n",
    "run_wilcoxon(mistral_f1_wikidata, alpaca_f1_wikidata, \"Mistral\", \"Alpaca-LoRA-13B\", \"Wikidata\")\n",
    "run_wilcoxon(mistral_f1_wikidata, vicuna_f1_wikidata, \"Mistral\", \"Vicuna\", \"Wikidata\")\n",
    "\n",
    "print(\"\\n=== Wilcoxon Signed-Rank Test on Baseline Evaluation (avg_f1 across domains) ===\")\n",
    "run_wilcoxon(llama3_f1_baseline, alpaca_f1_baseline, \"LLama 3-8B\", \"Alpaca-LoRA-13B\", \"Baseline\")\n",
    "run_wilcoxon(llama3_f1_baseline, vicuna_f1_baseline, \"LLama 3-8B\", \"Vicuna\", \"Baseline\")\n",
    "run_wilcoxon(mistral_f1_baseline, alpaca_f1_baseline, \"Mistral\", \"Alpaca-LoRA-13B\", \"Baseline\")\n",
    "run_wilcoxon(mistral_f1_baseline, vicuna_f1_baseline, \"Mistral\", \"Vicuna\", \"Baseline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e881817c-4b69-41dd-92a7-298b3cbccd16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KG Pipeline (GPU)",
   "language": "python",
   "name": "kg_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
